---
title: "Math 514"
subtitle: "TD-Gammon"
author: "Sally Matson, Fiona Paine"
date: "(updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    css: ["default", "default-fonts", "test.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      beforeInit: "macros.js"
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
# find.image searches subdirectories for partial match of argument. Useful when some images are dynamically generated
find.image=function(name){grep(name,list.files(recursive = TRUE),value=TRUE)}

# Example of how to use find.image to display an image
#![:scaleimgpct 100%](`r find.image("mnist_digits")`)

```

# TD Gammon 

TD Gammon is a a neural-network based algorithm that plays backgammon at a near expert level.
--

- Developed by Gerald Tesauro in 1992
--

- Uses temoral-difference learning & eligibility trace to update
--

- Begins with no strategy knowledge

---

# Backgammon Rules
## Fiona todo
---

# The Algorithm 

The neural network is a *value function* that should be able to predict, given any board configuration, the realtive advantage of each player.
--

### Heuristics:
--

- We only know the *actual* relative advantage at the end of the game (win = 1, loss = 0.)
--

  + **Idea**: At the end of the game we can calculate error: 1 - expected value. Can we backpropegate this? 
--

- Yes. However, moves leading up to the end of the game influence the later position -- you can't just back propegate error once at the end. 
--

  + **Problem**: How can we continually update the value function, not just once at the end, even though we don't have an absolute value unless it's a win or loss? 

---

# The Algorithm (cont.)

### Backprop weight updates:

 .content-box-blue[
  $$W_{t+1} = W_{t} + \alpha \frac{\delta C_t}{\delta W_t} \\ b_{t+1} = b_{t} + \alpha \frac{\delta C_t}{\delta b_t}$$ where *t* is an increment of time. 
 ]

--

### Issue 
Only take into account current gradient. 
---

# Eligibility Traces


 .content-box-blue[
  $$e_t = \lambda e_{t-1} + \frac{\delta C_t}{\delta \theta_t}$$
  ]

---

# Eligibility Traces (cont.)

### Let's unpack that:

.content-box-blue[
$$\\ e_0 = 0 \\ e_1 = 0 + \frac{\delta C_1}{\delta \theta_1} \\ e_2 = \lambda(\frac{\delta C_1}{\delta \theta_1}) + \frac{\delta C_2}{\delta \theta_2} \\ e_3 = \lambda[\lambda(\frac{\delta C_1}{\delta \theta_1}) + \frac{\delta C_2}{\delta \theta_2}] + \frac{\delta C_3}{\delta \theta_3}$$
]

---

# Tuning

Now, we have an algorithm that takes into account the move history, using hyperparameter $\lambda$ to weigh recent moves more heavily. 
--


#### Using $\lambda = 0$, 
$$e_t = 0 *e_{t-1} + \frac{\delta C_t}{\delta \theta_t} = \frac{\delta C_t}{\delta \theta_t}$$
--

**Result**: Mostly looks at recent game states. 
--


#### Using $\lambda = 1$,
$$e_t = 1 *e_{t-1} + \frac{\delta C_t}{\delta \theta_t} = 1 * [1*(\frac{\delta C_1}{\delta \theta_1}) + \frac{\delta C_2}{\delta \theta_2}...] + \frac{\delta C_t}{\delta \theta_t} \\= \frac{\delta C_1}{\delta \theta_1} + \frac{\delta C_2}{\delta \theta_2} + ... + \frac{\delta C_n}{\delta \theta_n}$$
--

**Result**: Counts all game states equally. 

---


# The Algorithm (cont.)

### Error Value
 .content-box-blue[
  $$W_{t+1} = W_{t} + \alpha * e_t * \text{error}  \\ b_{t+1} = b_{t} + \alpha \frac{\delta C_t}{\delta b_t}$$ where *t* is an increment of time. 
 ]

### New weight updates:

 .content-box-blue[
  $$W_{t+1} = W_{t} + \alpha * e_t * \text{error}  \\ b_{t+1} = b_{t} + \alpha \frac{\delta C_t}{\delta b_t}$$ where *t* is an increment of time. 
 ]

--


---

# Our GUI 
## Fiona todo

---


# Current State of the Algorithm 
- Spent time working with TensorFlow, haven't gotten it to work 
- Implementing using Vanilla R 
  + Challenges: Use of Objects in R is new, want good quality/readable code 


---


# Plans
## Fiona & Sally todo

---





