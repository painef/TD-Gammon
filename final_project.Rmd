---
title: "TD-Gammon Implementation for MATH 514"
author: "Sally Matson and Fiona Paine"
date: "4/27/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
source("vanilla_r_nn.R")
source("agents.R")
source("nn-utils.R")
source("backgammon_board.R")
source("eval.R")
library(zeallot)
```

# Introduction
Our goal in this project was to implement a version of Tesuro's TD-Gammon, and tune the algorithm to see how well it could perform. TD-Gammon is a Neural Network that uses reinforcement learning to play the game backgammon. 

Backgammon is an interesting game for Artificial Intelligence. Trying to take into account all possible board configurations and all possible future dice rolls is way too computationally intensive.... the game has effective branching factor of about 400, which makes search methods like those used in chess and checkers are impossible.


### Background Research on TD-Gammon

* [Tesauro's Original Paper](http://modelai.gettysburg.edu/2013/tdgammon/pa4.pdf)

* [Stanford Paper comparing Td-gammon to a Baysean approach](http://cs229.stanford.edu/proj2013/MolinFlyhammarBidgol-UsingMachineLearningToTeachAComputerToPlayBackgammon.pdf)

* [Cornell Article with good pseudocode](https://www.cs.cornell.edu/boom/2001sp/Tsinteris/gammon.htm)

* [Helpful medium article with link to tensorflow explination](https://medium.com/jim-fleming/before-alphago-there-was-td-gammon-13deff866197)

* [Class assignment with some Java pseudocode](http://modelai.gettysburg.edu/2013/tdgammon/pa4.pdf)


### Neural Network Structure

#### Cost Function 

#### Features

#### Forward Propegation
```{r fwprop, echo=FALSE}
print.fun('fwd.prop')
```

#### Back Propegation
```{r bkprop, echo=FALSE}
print.fun('bk.prop')
```

#### Training
```{r bkprop, echo=FALSE}
print.fun('bk.prop')
```
### Agent Structure

#### Random Agent

#### AI Agent



# Training & Tuning

All of our networks are three layers (one hidden layer). We tuned using the following parameters:

+---------------+---------------+---------+----------+---------------------+---------------------+
| Agent Number  | Activation    | $\alpha$| $\lambda$| Hidden Layer Units  | Max epochs          |
+===============+===============+=========+==========+=====================+=====================+
| 1             | Sigmoid       | 0.1     | 0.8      | 40                  | 300,000             |
+---------------+---------------+---------+----------+---------------------+---------------------+
| 2             | Relu          | 0.1     | 0.8      | 40                  | 200,000             |
+---------------+---------------+---------+----------+---------------------+---------------------+
| 3             | Relu          | 0.3     | 0.8      | 40                  | 50,000              |
+---------------+---------------+---------+----------+---------------------+---------------------+
| 4             | Relu          | 0.1     | 0.2      | 40                  | 50,000              |
+---------------+---------------+---------+----------+---------------------+---------------------+
| 5             | Sigmoid       | 0.1     | 0.8      | 80                  | 50,000              |
+---------------+---------------+---------+----------+---------------------+---------------------+
| 6             | Sigmoid       | 0.1     | 0        | 40                  | 50,000              |
+---------------+---------------+---------+----------+---------------------+---------------------+
| 7             | Sigmoid       | 0.1     | 1        | 40                  | 50,000              |
+---------------+---------------+---------+----------+---------------------+---------------------+
| 8             | Sigmoid       | 0.1     | 0.5      | 40                  | 50,000              |
+---------------+---------------+---------+----------+---------------------+---------------------+
| 9             | Sigmoid       | 0.02    | 0.8      | 40                  | 50,000              |
+---------------+---------------+---------+----------+---------------------+---------------------+
| 10            | Relu          | 0.02    | 0.8      | 40                  | 50,000              |
+---------------+---------------+---------+----------+---------------------+---------------------+
| 11            | Relu          | 0.1/0.01| 0.8      | 40                  | 200,000             |
+---------------+---------------+---------+----------+---------------------+---------------------+


```{r load 1, echo=FALSE, cache=TRUE}
trained_agent.50000.1 = readRDS(file = "saved/1/trained_agent_50000_sigmoid_basic.rds")
trained_agent.100000.1 = readRDS(file="saved/1/trained_agent_100000_sigmoid_basic.rds")
trained_agent.150000.1 = readRDS(file="saved/1/trained_agent_150000_sigmoid_basic.rds")
trained_agent.200000.1 = readRDS(file="saved/1/trained_agent_200000_sigmoid_basic.rds")
trained_agent.300000.1 = readRDS(file="saved/1/trained_agent_300000_sigmoid_basic.rds")
```

```{r load 2, echo=FALSE, cache=TRUE}
trained_agent.50000.2 = readRDS(file="saved/2/trained_agent_50000_relu_basic_NEW.rds")
trained_agent.200000.2 = readRDS(file="saved/2/trained_agent_200000_relu_basic_NEW.rds")
```

```{r load 3, echo=FALSE, cache=TRUE}
trained_agent.10000.3 = readRDS(file="saved/3/trained_agent_10000_relu_largeLR.rds")
trained_agent.50000.3 = readRDS(file="saved/3/trained_agent_50000_relu_largeLR.rds")
```

```{r load 4, echo=FALSE, cache=TRUE}
trained_agent.50000.4 = readRDS(file="saved/4/trained_agent_50000_relu_lowLAMBDA.rds")
```

```{r load 5, echo=FALSE, cache=TRUE}
trained_agent.50000.5 = readRDS(file="saved/5/trained_agent_sigmoid_big_hidden_layer.rds")
```

```{r load 6, echo=FALSE, cache=TRUE}
trained_agent.50000.6 = readRDS(file="saved/6/trained_agent_sigmoid_0_lambda.rds")
```

```{r load 7, echo=FALSE, cache=TRUE}
trained_agent.50000.7 = readRDS(file="saved/7/trained_agent_sigmoid_1_lambda.rds")
```

```{r load 8, echo=FALSE, cache=TRUE}
trained_agent.50000.8 = readRDS(file="saved/8/trained_agent_sigmoid_5_lambda.rds")
```

```{r load 9, echo=FALSE, cache=TRUE}
trained_agent.50000.9 = readRDS(file="trained_agent_sigmoid_low_alpha.rds")
```

```{r load 10, echo=FALSE, cache=TRUE}
trained_agent.50000.10 = readRDS(file="trained_agent_relu_low_alpha.rds")
```

```{r load 11, echo=FALSE, cache=TRUE}
trained_agent_200000.11 = readRDS(file="trained_agent_relu_200000_basic_SMOL_at_end.rds")
```

```{r agent1 graphs, echo=FALSE, cache=TRUE}
'''

cost_hist_sig = c(trained_agent.50000.1$cost, trained_agent.100000.1$cost, trained_agent.150000.1$cost, trained_agent.200000.1$cost)
cost_hist_relu = c(trained_agent.10000.2$cost, trained_agent.50000.2$cost, trained_agent.100000.2$cost, trained_agent.150000.2$cost, trained_agent.200000.2$cost)

plot(0,0,xlim=c(0,length(cost_hist_sig)),ylim=c(0,0.01), type="n", ylab="", xlab="Epochs")
lines(lowess(seq(1,length(cost_hist_sig)),cost_hist_sig,1/1000000),col="red")
lines(lowess(seq(1,length(cost_hist_relu)),cost_hist_relu,1/1000000),col="blue")
title("(Smoothed) cost history")


eval_hist_sig = c(trained_agent.50000.1$eval_hist, trained_agent.100000.1$eval_hist, trained_agent.150000.1$eval_hist, trained_agent.200000.1$eval_hist)
eval_hist_relu = c(trained_agent.10000.2$eval_hist, trained_agent.50000.2$eval_hist, trained_agent.100000.2$eval_hist, trained_agent.150000.2$eval_hist, trained_agent.200000.2$eval_hist)

plot(0,0,xlim=c(0,length(eval_hist_sig)),ylim=c(0,1), type="n", ylab="", xlab="Epochs")
points(x=seq(1,length(eval_hist_sig)), y=eval_hist_sig, col="red")
points(x=seq(1,length(eval_hist_relu)), y=eval_hist_relu, col="blue")
abline(lm(unlist(eval_hist_sig) ~ seq(1,length(eval_hist_sig))), col="red")
abline(lm(unlist(eval_hist_relu) ~ seq(1,length(eval_hist_relu))), col="blue")
title("Evaluation during training against random opponent")

print(trained_agent.50000.1$time + trained_agent.100000.1$time + trained_agent.150000.1$time + trained_agent.200000.1$time)
'''
```

Player 1 (Relu activation, 200,000 epochs, $\alpha=0.1$ and $\lambda=0.8$) playing Player 2 (Sigmoid activation, 50,000 epochs, $\alpha=0.1$ and $\lambda=0.8$) for 500 games.
```{r playing against eachother, cache=TRUE}
evaluate(trained_agent.200000.2$agent,trained_agent.200000.1$agent,500)$p1
```



